{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<div style=\"color:#254E58;margin:0;font-size:48px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;overflow:hidden;font-weight:600;\"> Fine-tuning for the machine translation model </div>\n\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: #7B0F2D; background-color: #ffffff;\">CREATED BY: NGUYEN THI CAM LAI</h5>\n","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"font-family: Verdana; font-size: 30px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #0A64A2; background-color: #ffffff;\"><b>General </b> introduction </h2>\n\n**Problem:** Machine translation using Transformer architecture model\n\n**Project objectives:**  This project will train an existing [model](https://huggingface.co/Helsinki-NLP/opus-mt-en-vi) on a new dataset [new dataset](https://huggingface.co/datasets/mt_eng_vietnamese/viewer/iwslt2015-vi-en/train) by ***fine-tuning*** the `weights` and `hyperparameters`, to improve the accuracy and performance of the trained model\n\n**Training model:** (original model)\n\n- Model name: Helsinki-NLP/opus-mt-en-vi\n\n- Link (Hugging Face): https://huggingface.co/Helsinki-NLP/opus-mt-en-vi\n\n- Source (Github): https://github.com/Helsinki-NLP/OPUS-MT-app/\n\n**Training dataset:**\n- Dataset name: mt_eng_vietnamese (iwslt2015-en-vi)\n- Link (Hugging Face): https://huggingface.co/datasets/mt_eng_vietnamese/viewer/iwslt2015-vi-en/train\n\n","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"font-family: Verdana; font-size: 30px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #0A64A2; background-color: #ffffff;\"><b>Finetuning </b> process </h2>","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“Œ Before you begin, make sure you have all the necessary libraries installed!","metadata":{}},{"cell_type":"code","source":"! pip install -U git+https://github.com/huggingface/transformers.git\n! pip install -U git+https://github.com/huggingface/accelerate.git","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets evaluate sacrebleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Sign in to Hugging Face so you can upload and share your models. When prompted, enter your token to login","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Load `mt_eng_vietnamese` dataset\n","metadata":{}},{"cell_type":"markdown","source":"**Start by loading the `iwslt2015-en-vi` subset of the `mt_eng_vietnamese` dataset from the `datasets` library:**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset(\"mt_eng_vietnamese\",'iwslt2015-en-vi')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Split the dataset into a train and test set with the `train_test_split` method:","metadata":{}},{"cell_type":"code","source":"data = data[\"train\"].train_test_split(test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"train\"][2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Preprocess","metadata":{}},{"cell_type":"markdown","source":"**The next step is to load a `Helsinki-NLP/opus-mt-en-vi` tokenizer to process the English-VietNam language pairs:**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"Helsinki-NLP/opus-mt-en-vi\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The preprocessing function you want to create needs to:**\n\n* Prefix the input with a prompt so Helsinki-NLP/opus-mt-en-vi knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n* Tokenize the input (English) and target (VietNam) separately because you canâ€™t tokenize VietNam text with a tokenizer pretrained on an English vocabulary.\n* Truncate sequences to be no longer than the maximum length set by the max_length parameter.","metadata":{}},{"cell_type":"code","source":"source_lang = \"en\"\ntarget_lang = \"vi\"\nprefix = \"translate English to Vietnamese: \"\n\n\ndef preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n    return model_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To apply the preprocessing function over the entire dataset, use `datasets map` method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:**\n\n","metadata":{}},{"cell_type":"code","source":"tokenized_data = data.map(preprocess_function, batched=True)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now create a batch of examples using `DataCollatorForSeq2Seq`. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length:**","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Evaluate","metadata":{}},{"cell_type":"markdown","source":"**Use library `evaluate` to get the fastest model evaluation score:**","metadata":{}},{"cell_type":"code","source":"! pip install evaluate","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install sacrebleu","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"sacrebleu\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a function that passes your predictions and labels to compute to calculate the `SacreBLEU` score:**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Train","metadata":{}},{"cell_type":"markdown","source":"**Load `Helsinki-NLP/opus-mt-en-vi` with `AutoModelForSeq2SeqLM`:**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint) ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**At this point, only three steps remain:**\n\n* Define your training hyperparameters in `Seq2SeqTrainingArguments`. The only required parameter is output_dir which specifies where to save your model. Youâ€™ll push this model to the Hub by setting `push_to_hub=True `(you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the `Trainer` will evaluate the SacreBLEU metric and save the training checkpoint.\n* Pass the training arguments to `Seq2SeqTrainer` along with the model, dataset, tokenizer, data collator, and compute_metrics function.\n* Call `train()` to finetune your model.","metadata":{}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir = \"en_vi_translation_1\",\n    evaluation_strategy = \"epoch\",\n    learning_rate = 2e-05,\n    per_device_train_batch_size = 16,\n    per_device_eval_batch_size = 16,\n    seed = 42,\n    adam_epsilon=1e-08,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    weight_decay=0.01,\n    save_total_limit=3,\n    predict_with_generate=True,\n    lr_scheduler_type = 'linear', \n    num_train_epochs = 3,\n    #push_to_hub=True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_data[\"train\"],\n    eval_dataset=tokenized_data[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Test translation on new sentence","metadata":{}},{"cell_type":"code","source":"text_example_1 = \"Hi, call me Lai. I am studying Data Science at VNUHCM - University of Science.\"\ntext_example_2 = \"I'm here to assusage my enthusiasm for Natural language processing.\"\ntext_example_3 = \"Natural Language Processing (NLP) is a branch of artificial intelligence.\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\ntranslator = pipeline(\"translation\", model=\"en_vi_translation_1\")\ntrans_text = translator(text_example_1)\nprint(trans_text)\ntrans_text = translator(text_example_2)\nprint(trans_text)\ntrans_text = translator(text_example_3)\nprint(trans_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“Œ Save weight","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'fine_tuned_weights.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"font-family: Verdana; font-size: 30px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #0A64A2; background-color: #ffffff;\"><b>Project </b> result </h2>\n\n- This model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-vi](https://huggingface.co/Helsinki-NLP/opus-mt-en-vi) on the [mt_eng_vietnamese](https://huggingface.co/datasets/mt_eng_vietnamese) dataset. It achieves the following results on the evaluation set:\n\n    - Loss: 1.376056\n\n    - Bleu: 34.515300\n\n    - Gen Len: 27.230600\n    \n\n- The model after training is saved at: https://huggingface.co/ntclai/en_vi_translation_1\n","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"font-family: Verdana; font-size: 30px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #0A64A2; background-color: #ffffff;\"><b>Reference </b> </h2>\n\n- https://huggingface.co/docs/transformers/tasks/translation?fbclid=IwAR039-v3EKAUPU6hiim5iTAUoQtE2B_iK_5HY2U7ThR1HlJyeEb30PaUIOU\n\n- https://github.com/mariaviana21/fine-tuning-machine-translation","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"color:#254E58;margin:0;font-size:48px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;overflow:hidden;font-weight:600;\"> Thank you for watching! </div>","metadata":{}}]}